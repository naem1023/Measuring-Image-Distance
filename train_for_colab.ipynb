{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "name": "train_for_colab.ipynb의 사본",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "history_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0321cce9ae2f469abc3b23fd790c419f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_335ad15316204106af7fabbbc4170b07",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_047e844c32604f8da65d3d8f6fbfacda",
              "IPY_MODEL_442721e79d0646c2b24788e5d7cf64d5"
            ]
          }
        },
        "335ad15316204106af7fabbbc4170b07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "047e844c32604f8da65d3d8f6fbfacda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ec73abbcb26c4afca8604ed50dc4ebef",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 10306551,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 10306551,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8fa6d6cc89124ebeaae5230995515605"
          }
        },
        "442721e79d0646c2b24788e5d7cf64d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d24460cdbd81409098b61033c03b765c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 9.83M/9.83M [00:00&lt;00:00, 70.1MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_16313498a12e4670ae9927c0829e6b1a"
          }
        },
        "ec73abbcb26c4afca8604ed50dc4ebef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8fa6d6cc89124ebeaae5230995515605": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d24460cdbd81409098b61033c03b765c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "16313498a12e4670ae9927c0829e6b1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naem1023/Measuring-Image-Distance/blob/feat%2F1-nydata-prototype/train_for_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uv8S6JFWglQ"
      },
      "source": [
        "# Setting TPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VwPZnANWkrP"
      },
      "source": [
        "# Make sure to use TPU\n",
        "import os\n",
        "assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FU4g04oXAK9",
        "outputId": "745876f4-4cb7-4a63-c7f5-1b907c4450a8"
      },
      "source": [
        "# Installing PyTorch/XLA\n",
        "!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.8.1-cp37-cp37m-linux_x86_64.whl"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: cloud-tpu-client==0.10 in /usr/local/lib/python3.7/dist-packages (0.10)\n",
            "Requirement already satisfied: torch-xla==1.8.1 from https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.8.1-cp37-cp37m-linux_x86_64.whl in /usr/local/lib/python3.7/dist-packages (1.8.1)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (from cloud-tpu-client==0.10) (4.1.3)\n",
            "Requirement already satisfied: google-api-python-client==1.8.0 in /usr/local/lib/python3.7/dist-packages (from cloud-tpu-client==0.10) (1.8.0)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from oauth2client->cloud-tpu-client==0.10) (0.17.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from oauth2client->cloud-tpu-client==0.10) (0.2.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from oauth2client->cloud-tpu-client==0.10) (4.7.2)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from oauth2client->cloud-tpu-client==0.10) (1.15.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client->cloud-tpu-client==0.10) (0.4.8)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.26.3)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.30.0)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.0.1)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.0.4)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.23.0)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (20.9)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.53.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.12.4)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (56.1.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2018.9)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (4.2.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.0.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ien1hOxXO5L",
        "outputId": "530ba59f-e61e-4214-8f78-a83bb944afb8"
      },
      "source": [
        "# Import pytorch/xla\n",
        "import torch\n",
        "\n",
        "# imports the torch_xla package\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:TPU has started up successfully with version pytorch-1.8.1\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWHW2-BlYWDd"
      },
      "source": [
        "# make device\n",
        "device = xm.xla_device()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "0Ja5K0tNVQdu"
      },
      "source": [
        "# Train distance via colab\n",
        "## Import Module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "E-8yxPmKVQdw"
      },
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets, models, transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import h5py\n",
        "import scipy.io\n",
        "from skimage import io, transform\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import json\n",
        "import os"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "GDJYNuEVVQdx"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "oKi5PgogVQdx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e27896af-208a-49dd-8fa7-13563dfb9b51"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "data_path = 'gdrive/MyDrive/Colab Notebooks/nyu_depth_data_labeled.mat'\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "LUkb-qKBVQdx"
      },
      "source": [
        "## Define Dataset Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "f8_QkB4zVQdx"
      },
      "source": [
        "class NyDataset(Dataset):\n",
        "    \"\"\"Newyork Data\"\"\"\n",
        "\n",
        "    def __init__(self, root_dir, transform=None, x_point=10, y_point=10):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir (string):\n",
        "                모든 이미지가 존재하는 디렉토리 경로\n",
        "            transform (callable, optional):\n",
        "                샘플에 적용될 Optional transform\n",
        "            point (int):\n",
        "                이미즈 한 변의 point 개수\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.img_data_file = h5py.File(root_dir)\n",
        "        self.transform = transform\n",
        "        self.x_point = x_point\n",
        "        self.y_point = y_point\n",
        "        self.point = x_point * y_point\n",
        "\n",
        "        f = h5py.File(self.root_dir)\n",
        "\n",
        "        self.len = f['images'].shape[0]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len * self.point\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        if type(idx) is list:\n",
        "            converted_idx = np.array([(i // self.point, i % self.point) for i in idx])\n",
        "        elif type(idx) is int:\n",
        "            converted_idx = np.array([[idx // self.point, idx % self.point]])\n",
        "            # converted_idx = np.reshape(converted_idx, (converted_idx.shape[0], 1))\n",
        "\n",
        "        image = self.__get_image(self.root_dir, converted_idx[:, 0])\n",
        "        # raw_depth_image = self.__get_raw_depth(self.root_dir, converted_idx[:, 0])\n",
        "        depth_image = self.__get_depth(self.root_dir, converted_idx[:, 0])\n",
        "\n",
        "        depth_list, target_coordinate = self.get_depth_point(depth_image, converted_idx[:, 1])\n",
        "\n",
        "        sample = {\n",
        "            'image': image,\n",
        "            'target_coordinate': target_coordinate\n",
        "        }\n",
        "        #\n",
        "        # if self.transform:\n",
        "        #     sample = self.transform(sample)\n",
        "\n",
        "        return sample, depth_list\n",
        "\n",
        "    def get_depth_point(self, depth_image, idxes):\n",
        "        # Not coordinate of image, only order of training points.\n",
        "        positions = [ [idx % self.point // self.x_point, idx % self.point % self.x_point ] for idx in idxes]\n",
        "        x_interval = depth_image.shape[1] // self.x_point\n",
        "        y_interval = depth_image.shape[2] // self.y_point\n",
        "\n",
        "        depth = [ depth_image[0][pos[0] * x_interval][pos[1] * y_interval] for pos in positions ]\n",
        "        target_coordinate = [ [pos[0] * x_interval, pos[1] * y_interval] for pos in positions ]\n",
        "\n",
        "        depth = np.array(depth)\n",
        "\n",
        "        return depth, target_coordinate\n",
        "\n",
        "    def __get_raw_depth(self, root_dir, idx):\n",
        "        rawDepth = self.img_data_file['rawDepths'][idx] / 4.0\n",
        "        # return rawDepth\n",
        "        # rawDepth_ = np.empty([480, 640, 3])\n",
        "        # rawDepth_[:, :, 0] = rawDepth[:, :].T\n",
        "        # rawDepth_[:, :, 1] = rawDepth[:, :].T\n",
        "        # rawDepth_[:, :, 2] = rawDepth[:, :].T\n",
        "\n",
        "        # image = io.imread(rawDepth_ / 4.0)\n",
        "        return rawDepth\n",
        "\n",
        "    def __get_depth(self, root_dir, idx):\n",
        "        depth = self.img_data_file['depths'][idx] # (1, 640, 480)\n",
        "        # return depth\n",
        "        # depth_ = np.empty([480, 640, 1])\n",
        "        # depth_[:, :, 0] = depth[:, :].T\n",
        "        # depth_[:, :, 1] = depth[:, :].T\n",
        "        # depth_[:, :, 2] = depth[:, :].T\n",
        "        # depth_ = depth.T\n",
        "\n",
        "        transform_depth = depth.astype('float32') / 4.0\n",
        "        # image = io.imread(depth_ / 4.0)\n",
        "        return transform_depth\n",
        "\n",
        "    def __get_image(self, root_dir, idx):\n",
        "        img = self.img_data_file['images'][idx][0] # (3, 640, 480)\n",
        "        # return img\n",
        "        # img_ = np.empty([480, 640, 3])\n",
        "        # img_[:, :, 0] = img[0, :, :].T\n",
        "        # img_[:, :, 1] = img[1, :, :].T\n",
        "        # img_[:, :, 2] = img[2, :, :].T\n",
        "\n",
        "        transform_img = img.astype('float32') / 255.0\n",
        "        # img = img.astype('float32') / 255.0\n",
        "        # image = io.imread(imag_ / 255.0)\n",
        "        return transform_img"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "ReLB0BCIVQdz"
      },
      "source": [
        "## Define Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "FAznngR8VQdz"
      },
      "source": [
        "class Data:\n",
        "    def __init__(self, path):\n",
        "        self.ny_dataset = NyDataset(path)\n",
        "\n",
        "    def get_dataset(self, train_ratio=0.8):\n",
        "        # Set split length\n",
        "        train_len = int(len(self.ny_dataset) * train_ratio)\n",
        "        test_len = len(self.ny_dataset) - train_len\n",
        "\n",
        "        train_dataset, test_dataset = torch.utils.data.random_split(self.ny_dataset, [train_len, test_len])\n",
        "\n",
        "        train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
        "        test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "        return train_loader, test_loader\n",
        "\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "VTAfFx0GVQdz"
      },
      "source": [
        "## Define MIS model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "8gQqGLtwVQd0"
      },
      "source": [
        "class MIS(nn.Module):\n",
        "    \"\"\"Measuring Image Distance Model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, sub_sampling_ratio=16, width=480, height=640, model_selection='mobile'):\n",
        "        super(MIS, self).__init__()\n",
        "        self.sub_sampling_ratio = sub_sampling_ratio\n",
        "        self.width = width\n",
        "        self.height = height\n",
        "        self.model_selection = model_selection\n",
        "\n",
        "        size = (7, 7)\n",
        "        fc1_out = 128\n",
        "        fc2_out = int(fc1_out / 4)\n",
        "\n",
        "        self.feature_extractor = self.get_feature_extraction()\n",
        "        self.adaptive_max_pool = nn.AdaptiveMaxPool2d(size)\n",
        "        self.fc1 = nn.Linear(7 * 7 * self.extraction_size + 2, fc1_out)\n",
        "        self.fc2 = nn.Linear(fc1_out, fc2_out)\n",
        "        self.fc3 = nn.Linear(fc2_out, 1)\n",
        "\n",
        "        print('make MIS model')\n",
        "\n",
        "    def rol_pooling(self, output_map):\n",
        "        output = [self.adaptive_max_pool(out)[0] for out in output_map]\n",
        "\n",
        "        return output\n",
        "\n",
        "    def forward(self, sample):\n",
        "        x = sample[0] # image\n",
        "        target = sample[1] # target coordinate\n",
        "\n",
        "        x = self.feature_extractor(x)\n",
        "        x = self.adaptive_max_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        targetT = torch.transpose(target, 0, 1)\n",
        "\n",
        "        x_list = x.tolist()\n",
        "        targetT_list = targetT.tolist()\n",
        "\n",
        "        for i in range(x.shape[0]):\n",
        "            for target in targetT_list[i]:\n",
        "                x_list[i].append(target)\n",
        "        x = torch.tensor(x_list).cuda()\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.fc3(x)\n",
        "        output = F.softplus(x)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def get_feature_extraction(self):\n",
        "        \"\"\"Return network which produces feature map.\n",
        "        \"\"\"\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        if self.model_selection == 'vgg':\n",
        "            model = torchvision.models.vgg11(pretrained=True).to(device)\n",
        "            self.extraction_size = 512\n",
        "        elif self.model_selection == 'mobile':\n",
        "            model = torchvision.models.mobilenet_v3_small(pretrained=True).to(device)\n",
        "            self.extraction_size = 48\n",
        "\n",
        "        features = list(model.features)\n",
        "\n",
        "        # only collect layers with output feature map size (W, H) < 50\n",
        "        dummy_img = torch.zeros((1, 3, self.width, self.height)).float()  # test image array\n",
        "\n",
        "        req_features = []\n",
        "        output = dummy_img.clone().to(device)\n",
        "\n",
        "        for feature in features:\n",
        "            output = feature(output)\n",
        "            #     print(output.size()) => torch.Size([batch_size, channel, width, height])\n",
        "\n",
        "            # If size of convolution result is threshold, break.\n",
        "            if output.size()[2] < self.width // self.sub_sampling_ratio \\\n",
        "                    and output.size()[3] < self.height // self.sub_sampling_ratio:\n",
        "                break\n",
        "            req_features.append(feature)\n",
        "\n",
        "        faster_rcnn_feature_extractor = nn.Sequential(*req_features)\n",
        "        return faster_rcnn_feature_extractor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "6qniv5YTVQd0"
      },
      "source": [
        "## Define train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "qQmqsehFVQd0"
      },
      "source": [
        "class Train:\n",
        "    def __init__(self):\n",
        "        self.save_path = '.\\\\'\n",
        "\n",
        "    def train(self, data):\n",
        "        train_loader, test_loader = data\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        # torch.manual_seed(53)\n",
        "        # if device == 'cuda':\n",
        "        #     torch.cuda.manual_seed_all(53)\n",
        "\n",
        "        model = MIS()\n",
        "\n",
        "        if torch.cuda.device_count() > 1:\n",
        "            os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0, 1, 2, 3'\n",
        "            model = nn.DataParallel(model, output_device=1)\n",
        "\n",
        "        model = model.to(device)\n",
        "\n",
        "        # Optimize\n",
        "        # criterion = nn.CrossEntropyLoss().cuda()\n",
        "        criterion = nn.SmoothL1Loss().cuda()\n",
        "        # optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0005)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "        import time\n",
        "        start_time = time.time()\n",
        "        min_loss = int(1e9)\n",
        "        history = {'loss': [], 'val_acc': []}\n",
        "        for epoch in range(1):  # loop over the dataset multiple times\n",
        "            epoch_loss = 0.0\n",
        "            tk0 = tqdm(train_loader, total=len(train_loader), leave=False)\n",
        "            for step, (inputs, labels) in enumerate(tk0, 0):\n",
        "                image_inputs = inputs['image']\n",
        "                coordinate_inputs = torch.stack([val for val in inputs['target_coordinate'][0]], dim=0).to(device)\n",
        "                image_inputs, labels = image_inputs.to(device), labels.to(device)\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model((image_inputs, coordinate_inputs))\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "            # validation\n",
        "            if epoch % 10 == 0:\n",
        "                class_correct = list(0. for i in range(1000))\n",
        "                class_total = list(0. for i in range(1000))\n",
        "                with torch.no_grad():\n",
        "                    for data in test_loader:\n",
        "                        images, labels = data\n",
        "                        images = images.cuda()\n",
        "                        labels = labels.cuda()\n",
        "                        outputs = model(images)\n",
        "                        _, predicted = torch.max(outputs, 1)\n",
        "                        c = (predicted == labels).squeeze()\n",
        "                        for i in range(labels.size()[0]):\n",
        "                            label = labels[i].item()\n",
        "                            class_correct[label] += c[i].item()\n",
        "                            class_total[label] += 1\n",
        "                val_acc = sum(class_correct) / sum(class_total) * 100\n",
        "            else:\n",
        "                val_acc = 0\n",
        "\n",
        "            # print statistics\n",
        "            tqdm.write('[Epoch : %d] train_loss: %.5f val_acc: %.2f Total_elapsed_time: %d 분' %\n",
        "                       (epoch + 1, epoch_loss / 272, val_acc, (time.time() - start_time) / 60))\n",
        "            history['loss'].append(epoch_loss / 272)\n",
        "            history['val_acc'].append(val_acc)\n",
        "\n",
        "            if epoch in [36, 64, 92]:\n",
        "                for g in optimizer.param_groups:\n",
        "                    g['lr'] /= 10\n",
        "                print('Loss 1/10')\n",
        "\n",
        "        print(time.time() - start_time)\n",
        "        print('Finished Training')\n",
        "\n",
        "        torch.save(model.state_dict(), os.path.join(self.save_path, 'model_state_dict.pt'))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "Njx8KARvVQd1"
      },
      "source": [
        "# Load Data and Run trainer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "wnAxuq9mVQd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195,
          "referenced_widgets": [
            "0321cce9ae2f469abc3b23fd790c419f",
            "335ad15316204106af7fabbbc4170b07",
            "047e844c32604f8da65d3d8f6fbfacda",
            "442721e79d0646c2b24788e5d7cf64d5",
            "ec73abbcb26c4afca8604ed50dc4ebef",
            "8fa6d6cc89124ebeaae5230995515605",
            "d24460cdbd81409098b61033c03b765c",
            "16313498a12e4670ae9927c0829e6b1a"
          ]
        },
        "outputId": "c68baf57-42cc-41c3-a5eb-1e15295568fc"
      },
      "source": [
        "dataset = Data(data_path)\n",
        "train_data = dataset.get_dataset()\n",
        "\n",
        "trainer = Train()\n",
        "trainer.train(train_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v3_small-047dcff4.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v3_small-047dcff4.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0321cce9ae2f469abc3b23fd790c419f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=10306551.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/2855 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "make MIS model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 23%|██▎       | 660/2855 [14:52<47:52,  1.31s/it]"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}