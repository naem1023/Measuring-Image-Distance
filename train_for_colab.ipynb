{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Train distance via colab\n",
    "## Import Module"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import h5py\n",
    "import scipy.io\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "data_path = 'gdrive/MyDrive/Colab Notebooks/nyu_depth_data_labeled.mat'\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define Dataset Loader"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class NyDataset(Dataset):\n",
    "    \"\"\"Newyork Data\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, transform=None, x_point=10, y_point=10):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string):\n",
    "                모든 이미지가 존재하는 디렉토리 경로\n",
    "            transform (callable, optional):\n",
    "                샘플에 적용될 Optional transform\n",
    "            point (int):\n",
    "                이미즈 한 변의 point 개수\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.img_data_file = h5py.File(root_dir)\n",
    "        self.transform = transform\n",
    "        self.x_point = x_point\n",
    "        self.y_point = y_point\n",
    "        self.point = x_point * y_point\n",
    "\n",
    "        f = h5py.File(self.root_dir)\n",
    "\n",
    "        self.len = f['images'].shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len * self.point\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        if type(idx) is list:\n",
    "            converted_idx = np.array([(i // self.point, i % self.point) for i in idx])\n",
    "        elif type(idx) is int:\n",
    "            converted_idx = np.array([[idx // self.point, idx % self.point]])\n",
    "            # converted_idx = np.reshape(converted_idx, (converted_idx.shape[0], 1))\n",
    "\n",
    "        image = self.__get_image(self.root_dir, converted_idx[:, 0])\n",
    "        # raw_depth_image = self.__get_raw_depth(self.root_dir, converted_idx[:, 0])\n",
    "        depth_image = self.__get_depth(self.root_dir, converted_idx[:, 0])\n",
    "\n",
    "        depth_list = self.get_depth_point(depth_image, converted_idx[:, 1])\n",
    "\n",
    "        # sample = {\n",
    "        #     'image': image,\n",
    "        #     'raw_depth_image': raw_depth_image,\n",
    "        #     'depth_image': depth_image,\n",
    "        # }\n",
    "        #\n",
    "        # if self.transform:\n",
    "        #     sample = self.transform(sample)\n",
    "\n",
    "        return image, depth_list\n",
    "\n",
    "    def get_depth_point(self, depth_image, idxes):\n",
    "        # Not coordinate of image, only order of training points.\n",
    "        positions = [ [idx % self.point // self.x_point, idx % self.point % self.x_point ] for idx in idxes]\n",
    "        x_interval = depth_image.shape[1] // self.x_point\n",
    "        y_interval = depth_image.shape[2] // self.y_point\n",
    "\n",
    "        depth = [ depth_image[0][pos[0] * x_interval][pos[1] * y_interval] for pos in positions ]\n",
    "        # Range of depth is 0 to 10. So divide by 10.\n",
    "        depth = np.array(depth)\n",
    "        # depth = depth.astype(np.int64)\n",
    "\n",
    "        return depth\n",
    "\n",
    "    def __get_raw_depth(self, root_dir, idx):\n",
    "        rawDepth = self.img_data_file['rawDepths'][idx] / 4.0\n",
    "        # return rawDepth\n",
    "        # rawDepth_ = np.empty([480, 640, 3])\n",
    "        # rawDepth_[:, :, 0] = rawDepth[:, :].T\n",
    "        # rawDepth_[:, :, 1] = rawDepth[:, :].T\n",
    "        # rawDepth_[:, :, 2] = rawDepth[:, :].T\n",
    "\n",
    "        # image = io.imread(rawDepth_ / 4.0)\n",
    "        return rawDepth\n",
    "\n",
    "    def __get_depth(self, root_dir, idx):\n",
    "        depth = self.img_data_file['depths'][idx] # (1, 640, 480)\n",
    "        # return depth\n",
    "        # depth_ = np.empty([480, 640, 1])\n",
    "        # depth_[:, :, 0] = depth[:, :].T\n",
    "        # depth_[:, :, 1] = depth[:, :].T\n",
    "        # depth_[:, :, 2] = depth[:, :].T\n",
    "        # depth_ = depth.T\n",
    "\n",
    "        transform_depth = depth.astype('float32') / 4.0\n",
    "        # image = io.imread(depth_ / 4.0)\n",
    "        return transform_depth\n",
    "\n",
    "    def __get_image(self, root_dir, idx):\n",
    "        img = self.img_data_file['images'][idx][0] # (3, 640, 480)\n",
    "        # return img\n",
    "        # img_ = np.empty([480, 640, 3])\n",
    "        # img_[:, :, 0] = img[0, :, :].T\n",
    "        # img_[:, :, 1] = img[1, :, :].T\n",
    "        # img_[:, :, 2] = img[2, :, :].T\n",
    "\n",
    "        transform_img = img.astype('float32') / 255.0\n",
    "        # img = img.astype('float32') / 255.0\n",
    "        # image = io.imread(imag_ / 255.0)\n",
    "        return transform_img"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Data:\n",
    "    def __init__(self, path):\n",
    "        self.ny_dataset = Dataset(path)\n",
    "\n",
    "    def get_dataset(self, train_ratio=0.8):\n",
    "        # Set split length\n",
    "        train_len = int(len(self.ny_dataset) * train_ratio)\n",
    "        test_len = len(self.ny_dataset) - train_len\n",
    "\n",
    "        train_dataset, test_dataset = torch.utils.data.random_split(self.ny_dataset, [train_len, test_len])\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=16, shuffle=True)\n",
    "        test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "        return train_loader, test_loader\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define MIS model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MIS(nn.Module):\n",
    "    \"\"\"Measuring Image Distance Model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sub_sampling_ratio=16, width=480, height=640):\n",
    "        super(MIS, self).__init__()\n",
    "        self.sub_sampling_ratio = sub_sampling_ratio\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        size = (7, 7)\n",
    "\n",
    "        self.feature_extractor = self.get_feature_extraction()\n",
    "        self.adaptive_max_pool = nn.AdaptiveMaxPool2d(size)\n",
    "        self.fc1 = nn.Linear(7 * 7 * 512, 2048)\n",
    "        self.fc2 = nn.Linear(2048, 512)\n",
    "        self.fc3 = nn.Linear(512, 1)\n",
    "\n",
    "        print('make MIS model')\n",
    "\n",
    "    def rol_pooling(self, output_map):\n",
    "        output = [self.adaptive_max_pool(out)[0] for out in output_map]\n",
    "\n",
    "        return output\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.adaptive_max_pool(x)\n",
    "        # x = torch.cat(x, 0)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        output = F.softplus(x)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def get_feature_extraction(self):\n",
    "        \"\"\"Return network which produces feature map.\n",
    "        \"\"\"\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        model = torchvision.models.vgg16(pretrained=True).to(device)\n",
    "        features = list(model.features)\n",
    "\n",
    "        # only collect layers with output feature map size (W, H) < 50\n",
    "        dummy_img = torch.zeros((1, 3, self.width, self.height)).float()  # test image array\n",
    "\n",
    "        req_features = []\n",
    "        output = dummy_img.clone().to(device)\n",
    "\n",
    "        for feature in features:\n",
    "            output = feature(output)\n",
    "            #     print(output.size()) => torch.Size([batch_size, channel, width, height])\n",
    "\n",
    "            # If size of convolution result is threshold, break.\n",
    "            if output.size()[2] < self.width // self.sub_sampling_ratio \\\n",
    "                    and output.size()[3] < self.height // self.sub_sampling_ratio:\n",
    "                break\n",
    "            req_features.append(feature)\n",
    "\n",
    "        faster_rcnn_feature_extractor = nn.Sequential(*req_features)\n",
    "        torch.cuda.empty_cache()\n",
    "        return faster_rcnn_feature_extractor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Train:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train(self, data):\n",
    "        train_loader, test_loader = data\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # torch.manual_seed(53)\n",
    "        # if device == 'cuda':\n",
    "        #     torch.cuda.manual_seed_all(53)\n",
    "\n",
    "        model = ny.ml_model.MIS()\n",
    "\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0, 1, 2, 3'\n",
    "            model = nn.DataParallel(model, output_device=1)\n",
    "\n",
    "        model = model.to(device)\n",
    "\n",
    "        # Optimize\n",
    "        # criterion = nn.CrossEntropyLoss().cuda()\n",
    "        criterion = nn.SmoothL1Loss().cuda()\n",
    "        # optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0005)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        min_loss = int(1e9)\n",
    "        history = {'loss': [], 'val_acc': []}\n",
    "        for epoch in range(1):  # loop over the dataset multiple times\n",
    "            epoch_loss = 0.0\n",
    "            tk0 = tqdm(train_loader, total=len(train_loader), leave=False)\n",
    "            for step, (inputs, labels) in enumerate(tk0, 0):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            # validation\n",
    "            if epoch % 10 == 0:\n",
    "                class_correct = list(0. for i in range(1000))\n",
    "                class_total = list(0. for i in range(1000))\n",
    "                with torch.no_grad():\n",
    "                    for data in test_loader:\n",
    "                        images, labels = data\n",
    "                        images = images.cuda()\n",
    "                        labels = labels.cuda()\n",
    "                        outputs = model(images)\n",
    "                        _, predicted = torch.max(outputs, 1)\n",
    "                        c = (predicted == labels).squeeze()\n",
    "                        for i in range(labels.size()[0]):\n",
    "                            label = labels[i].item()\n",
    "                            class_correct[label] += c[i].item()\n",
    "                            class_total[label] += 1\n",
    "                val_acc = sum(class_correct) / sum(class_total) * 100\n",
    "            else:\n",
    "                val_acc = 0\n",
    "\n",
    "            # print statistics\n",
    "            tqdm.write('[Epoch : %d] train_loss: %.5f val_acc: %.2f Total_elapsed_time: %d 분' %\n",
    "                       (epoch + 1, epoch_loss / 272, val_acc, (time.time() - start_time) / 60))\n",
    "            history['loss'].append(epoch_loss / 272)\n",
    "            history['val_acc'].append(val_acc)\n",
    "\n",
    "            if epoch in [36, 64, 92]:\n",
    "                for g in optimizer.param_groups:\n",
    "                    g['lr'] /= 10\n",
    "                print('Loss 1/10')\n",
    "\n",
    "        print(time.time() - start_time)\n",
    "        print('Finished Training')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Data and Run trainer\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = Data(data_path)\n",
    "train_data = dataset.get_dataset()\n",
    "\n",
    "trainer = Train()\n",
    "trainer.train(train_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}